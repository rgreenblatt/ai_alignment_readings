# ai_alignment_readings

## Meta sources:

[https://www.eacambridge.org/agi-safety-fundamentals](https://www.eacambridge.org/agi-safety-fundamentals)

## General background I found non obvious and useful

Note, assumes you already buy that AI alignment is serious issue etc. Weakly assumes background on ideas
such as in Superintelligence by Bostrom.

 - [bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html):
   discusses how computation has been limiting factor in practice
 - [prosaic ai](https://ai-alignment.com/prosaic-ai-control-b959644d79c2): idea
   that scaling up current DL (particularly language models) could result in AGI
 - [80000 hours with Paul Christiano](https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/): 
   a bit old. Reflects Paul's thoughts on likely time lines and scenarios (not 'brain in a box', slower takeoff). 
 - maybe various models of intelligence/superintelligence (I currently don't think this is very important)
   - e.g.: [https://www.alignmentforum.org/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services-as](https://www.alignmentforum.org/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services-as)

## Concepts you should know about
 - POMDPs (and other agent models?)
 - Scaling laws
 - Various current research avenues
   - interpretability
   - deceptiveness (particularly language models)
   - alignment strategies
     - summary of [iterated amplification](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616)
       maybe [see also](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616)
     - week [4 (how to learn from humans?)](https://www.eacambridge.org/agi-week-4) 
     + [5 (Decomposing tasks for outer alignment)](https://www.eacambridge.org/agi-week-5) of EA Cambridge course (TODO: pick subset)
   - general theory (MIRI stuff, etc.)
 - timelines: [decent guide](https://www.cold-takes.com/where-ai-forecasting-stands-today/)
 - [orthogonality thesis and instrumental convergence](https://www.nickbostrom.com/superintelligentwill.pdf)
 - concerns/issues with current approaches (deep learning, etc.)
   - [specification gaming/reward hacking](https://deepmindsafetyresearch.medium.com/specification-gaming-the-flip-side-of-ai-ingenuity-c85bdb0deeb4)
   - [risks from learned optimization](https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB)
     - mesa optimization
     - [gradient hacking](https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking)
 - [tool vs agent AIs](https://www.gwern.net/Tool-AI) (and why we are likely to end up with agent AIs
 - [week 6 (other paradigms for safety work)](https://www.eacambridge.org/agi-week-6) of EA Cambridge course (TODO: subset)
 - AI governance
   - [week 7](https://www.eacambridge.org/agi-week-7) of EA Cambridge course (TODO: subset)
   - TODO more stuff here
 - 
## Career advice
 - [via Rohin Shah](https://rohinshah.com/faq-career-advice-for-ai-alignment-researchers/)
