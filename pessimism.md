EY (and many people at MIRI) are very pessimistic on alignment.

Specifically my understanding is they think (and have thought for a while) that
prosaic alignment isn't going to work.  A while ago they thought that there
might be some non-prosaic alignment approaches which work. This was what MIRI
was (is?) working on: agent foundations and some other things.  However, now EY
thinks that this research is doomed and that in turn implies that humanity is
doomed.  

(Ok, not exactly and there is some nuance around beliefs about arms
races and societal coordination, but the general view is that probability of
success is low and we need to reexamine the situation and try to find higher EV
paths.)

![text](https://i.kym-cdn.com/entries/icons/original/000/018/012/this_is_fine.jpeg) 

Reading:
- [Nate's thoughts on a recent report from Carlsmith at open phil](https://www.alignmentforum.org/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential) 
  (also take a look at report itself and note that nate is roughly in this MIRI school of thought)
- [Discussion with Eliezer Yudkowsky on AGI interventions](https://www.lesswrong.com/posts/CpvyhFy9WvCNsifkY/discussion-with-eliezer-yudkowsky-on-agi-interventions)
  The comments of decently high karma are all pretty important. I would
  probably read all of them and prioritize comments over reading discussion in
  full depth (but you need to at least skim discussion). The comments talk about
  possibility of formal verification and objections to the views of EY.
- [follow up post: 'What would we do if alignment were futile?'](https://www.lesswrong.com/posts/Xv77XjuZEkjRsvkJp/what-would-we-do-if-alignment-were-futile)

I am considerably less pessimistic. Maybe 10-40% chance of AI catastrophe which
results in losing most of attainable utility in the next 50 years? (Carlsmith
puts this at >5% and Nate puts this ~75%, but note that they are answering a
slightly different question).

I also tend to think that some current prosaic alignment work is reasonably
promising and that reasonably naive myopic approaches might be reasonably safe.
Then a lot of questions come down to societal competence/coordination.
