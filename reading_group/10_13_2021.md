We ended up talking about:
 - [Risks from learned optimization](https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB)
     - this is medium to long
     - maybe start by watching [this youtube video](https://www.youtube.com/watch?v=bJLcIBixGj8).
       It might be a better broad explanation depending on your tastes.
     - It's certainly ok to skim in parts, but make sure you understand the
       concepts of mesa-optimization and deceptive alignment. These ideas are
       referenced in a lot of places 
     - Also, make sure you reason about how plausible you think this sort of
       issue is. With current learning techniques? With future unknown
       techniques?
 - Take a look at [ai and compute](https://openai.com/blog/ai-and-compute/) and
   [ai and efficiency](https://openai.com/blog/ai-and-efficiency/). Just think
   this is interesting and pretty quick (not really very important background)
 - [11 Proposals](https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai)
   I think its ok to skim this in parts.
 - Going forward we will pick some approach to AI safety to learn about and discuss. Also
   share what you are going to talk about before we end up meeting so everyone
   else can read a bit about it. Working with someone else is fine and probably
   a good idea. Here are some ideas (might be good to read more than just what
   I have linked here if you choose one of these topics):
     - Assistance games
       https://www.alignmentforum.org/posts/qPoaA5ZSedivA4xJa/our-take-on-chai-s-research-agenda-in-under-1500-words
       and cooperative inverse reinforcement learning
       https://arxiv.org/abs/1606.03137
     - Iterated Distillation and Amplification:
       https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616
     - AI safety via debate https://openai.com/blog/debate/
 - Ryan and Thomas ended up talking about [recursive reward modeling](https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84) at this meeting
